<style>code.block{display:block;font-family:monospace;}</style>
<p>Since its publication in 1993, Martha Mitchell's 629 page <i>Encyclopedia Brunoniana</i> has served as the definitive reference work of Brown University's history. Its 668 articles document the University's <a href="../Topics/Buildings">buildings</a>, <a href="../Topics/Departments">departments</a>, <a href="../Topics/People">people</a>, and <a href="../Topics/Publications">publications</a>. The <i>Liber Brunoniana</i> project utilizes natural language processing techniques to transform Mitchell's text into hypertext, automatically inferring over 5,000 hyperlinks between articles, deliminating content into categories, and constructing pages detailing the events of <a href="./Topics/Years">each year</a> mentioned in the encyclopedia. This article details the techniques used to construct <i>Liber Brunoniana</i>, a book freed from the limitations of paper.</p>

<section><h2>Aquisition</h2> 
  <p>The possibility of creating <i>Liber Brunoniana</i> owes itself to Brown's longstanding distribution of a basic <a href="http://www.brown.edu/Administration/News_Bureau/Databases/Encyclopedia/">online edition of Encyclopedia Brunoniana</a>. We used a <a href="https://github.com/liber-brunoniana/scraper">simple python</a> script to scrape the article text of this online edition. The faithful rendition of the text into mostly semantic HTML (blockquotes are enclosed in the appropriate tag, for example), eased the subsequent steps of transforming the text using rule-based natural language processing, and transforming the markup for our presentation.</p> 
</section>

<section><h2>Transformation</h2>
  <p>Nonwithstanding presentation, the current online edition of <i>Encyclopedia Brunoniana</i> is an effective transformation of a text into HTML, but a poor case-study on the enrichment of a document with hypertext. This fault is particularly jarring since hypertext was designed not for creating applications (as is now the trend), but for organizing and presenting vast amounts of organized documents. The only navigation mechanism provided by the current online edition is index on its home page of over six hundred hyperlinks. Index-based navigation is suitable for printed books because books afford the user the ability to browse with the mere flip of a page. When the ability to browse is removed, index-based navigation remains suitable only for users who have a precise quarry in mind.</p>
  
  <p>For insight on what an effective rendition of the encyclopedic form in hypertext entailed, we looked to none other than Wikipedia. The English edition of the site effectively presents over five-million articles; a volume for which a print rendition would be unfeasible! While the number of articles in <i>Encyclopedia Brunoniana</i> doesn't prohibit offering a <a href=".">definitive index of articles</a>, it's not so few that more expressive navigation mechanisms aren't useful. <i>Liber Brunoniana</i> borrows Wikipedia's classification of articles into categories (including the ability to classify categories themselves into categories) and inter-document navigation via wikilinks. For technical reasons, we haven't yet implemented an integrated document search, but I suspect the necessity of search is diminished for collections of under a thousand documents. Finally, we borrowed Wikipedia's practice of thematic meta-pages, namely that of <a href="https://en.wikipedia.org/wiki/Wikipedia:WikiProject_Time">year pages</a>, which summarize events of a given year.</p>
  
  <section><h3>Iterative Classification</h3>
  <p>We initially tried to apply the clustering techniques detailed in Brandon Rose's<i><a href="http://brandonrose.org/clustering">Document Clustering in Python</a></i>, but the results were, from a glance, underwhelming. That such an evaluation could be made at a glance owed itself to the uniform structures present in Martha Mitchell's choice of article titles. For many categories, we were able to generate simple rules that matched precisely the set of articles we desired. Articles about <a href="../Topics/People">people</a>, for example, have titles following the structure "Last, First M.I". Likewise, articles about Brown's <a href="../Topics/Gates">gates</a> contained the string "Gate" in their title.</p>
  
  <p>The number of categories that could be derrived with total accuracy from title alone was very small, but the certainty and success of the process enabled us to iteratively bootstrap structured semantics onto the text. Articles about buildings, for example, invariably contain the phrase "built in", but so did other types of articles: the article about a building's namesake often references the structure that memorialized them, and non-building articles such as gates contain the phrase "built in", too. To create a category containing all articles about buildings, we searched the set of articles which had not already been placed into the "people" or "gates" categories for the text "built in".</p>
  
  <p>Similarly, to construct the sub-category <a href="../Topics/People/Professors">Professors</a>, we filtered for the phrase "professor of" within articles that had already been categorized as people.</p>
  
  <p>By iteratively structuring the document collection, we increased the precision of classificaton, without increasing the complexity of performing it. No surefire regular expression identifies <a href="../Topics/Publications">Publications</a> with few false positives, but we were able to continue to use very general search expressions by reducing the search space with prior classifications</p>
  </section>
  
  <section><h3>Entity Linking</h3>
  <p>To create the same sort of experience of exploration that makes sites like Wikipedia or TVTropes addicting to navigate, we attempted to automatically identify keywords in articles that corresponded to other articles replace them with hyperlinks. The same structured properties of Martha Mitchell's titles that enabled classification simplified entity linking, too.</p>
  
  <p>To identify keywords, we simply performed a case-sensitive search of each article's text for the names of other articles. This na√Øve technique performed surprisingly well. Applied directly to a collection like Wikipedia, such a process would flood articles with irrelevant hyperlinks (which is to say nothing of the problem of disambiguation), but it proves suitable for collections of documents with narrow breadth and uncommon names. The only problematic article in Encyclopedia Brunoniana was <a href="./Well.html">Well</a>.</p>
  
  <p>While a regular expression powered search-and-replaced powered the initial attempt at entity linking, a common, confounding case rendered it useless. Brown&mdash;and by extension, Encyclopedia Brunoniana&mdash;has a history rich with countless notable individuals, whom it honors by giving buildings their name. Thus to "<a href="./Hay, John.html">John Hay</a>", add the "<a href="./John Hay Library">John Hay Library</a>", and so on. With regular expressions alone, it is impossible to express that a hyperlink should never be nested inside another hyperlink; to express this, we enter the realm of context-free languages. Consistently handling these cases threatened to explode the complexity of the task into parsing HTML. Greg Hendershott's <a href="https://github.com/greghendershott/frog/blob/master/frog/xexpr-map.rkt">xexpr-map</a> procedure reduced the challenge of expressing a context-aware tree transformer to a few lines of Racket:</p>
  <script src="https://gist.github.com/jswrenn/4ec603f54271baea9c3f.js"></script>
  <p>For all articles, we perform linkification with an identity mapping between article name and keyword. For articles about people (which we can identify with absolute certainty), additionally linkify with various common arrangements of name components.</p>
  
  <p>Excluding links introduced by categories and year pages, this process introduced over 3,000 hyperlinks between documents. Visualized, this process reflects the transformation of a disparate cloud of about six-hundred-eighty articles,</p>
  <img src="https://i.imgur.com/he3y37L.png" width="100%" height="auto"></img>
  <p>...into a complex web that leaves few documents orphaned (if you include hyperlinks introduced by date pages, there are no orphaned pages):</p>
  <img src="https://i.imgur.com/7NE3Ao1.png" width="100%" height="auto"></img>
  <p>That not sufficient evidence to say this is a functional improvement, but by exploring Liber Brunoniana you can be the judge of that.</p>
  </section>
  
  <section><h3>Date Fact Extraction</h3>
  <p>Generating date pages (like the one for <a href="./1828.html">1828</a>), also benefited from being able to confidently identify pages about people. Generally speaking, the datification process consisted of tokenizing articles into lists of sentences, filtering out all sentences not containing four digit numbers, and adding the remainder to a date fact database from which date pages are created. Two challenges arose: Sentence tokenization, despite great support from Python's NLTK library, was confounded by the glut of esoteric abbreviations (mostly related to various degrees) whose periods were mistaken as sentence terminators. Fortunately, the tokenizer could easily be extended to recognize <a href="https://github.com/liber-brunoniana/dateify/blob/master/datefacts.py#L8">additional abbreviations</a>.</p>
  
  <p>The not-unexpected second challenge was disambiguating sentences that identified their subjects only by pronoun. While such sentences are perfectly acceptable with context, all date-facts are one-sentence fragments from articles. For articles about people, we know the likely subject of any pronoun, and replaced prounouns with the person's name. We avoided, importantly, replacing pronouns in sentences that already contained the subject's name. Such ambiguous sentences are fortunately much less common in articles not about people and we do not attempt to dereference any pronouns encountered there. While some complex disambiguation mechanism might be feasible, an ambiguous date fact is preferable to a date fact rendered unambiguously erroneous. All date facts are followed by a citation to the article they were extracted from so that the reader can learn more if they wish.</p>
  </section>
</section>

<section><h2>Construction</h2>
  
  <section><h3>Stiki: The Static Wiki</h3>
  <p>Static site generators are enjoying a rennaisance in popularity for their ease of reasoning about, and their properties of consistent performance and resource consumption. Much of this rennaisance has been in the area of static blogs, and in trying replicate the functionality of a blog some prepackaged generated are quite complex. We believed that the technique of static site generation would be an even more natural fit to the domain of online wikis. Moreover, we believed that such a generator could match the functionality of a dynamic wiki while operating on simple principals and mechanics. Although 'static' may seem antithetical to the collaborative nature of a Wiki, it actually delegates those responsibilities to more qualified agents. Liber Brunoniana delegates the responsibility of collaboration and change-tracking to Git, and webhook-triggered build scripts render changes within seconds.</p>

  <p>Stiki, the static wiki, is a simple static site generator for wiki-like sites that derives complex functionality from the consistent application of two principals:
  <ul>
    <li>Files are pages.</li>
    <li>Folders are categories.</li>
  </ul>
  The simplicity of these rules belies their flexibility. The 1-1 mapping between the filesystem and the generated site imbues Stiki-powered sites with all the functionality and flexibility of the filesystem, a proven document organization mechanism. Just as symbolic links, for example, enable a filesystem entity to exist in more than one place, it allows articles and categories to exist in multiple categories.</p>

  <p>Of any document management system, filesystems have the best tooling available. Stiki leverages this tooling to express relationships between documents extremely tersely. To list all of the categories an article or category belongs to:
  <code class="block">find -L . -samefile "$1" -print0 | dirname -z</code>
  To list all sub-pages of a category:
  <code class="block">find "$1" -mindepth 1 -maxdepth 1 -xtype f</code>
  To list all sub-categories of a category:
  <code class="block">find "$1" -mindepth 1 -maxdepth 1 -xtype d</code>
  In these three commands, we've expressed the necessary relationshps to create a encyclopedic-like site.</p>

  <p>Stiki is less a piece of software than it is a set of principals. The initial, slightly unwieldy, Racket-powered generator created for this project occupies the <a href="https://github.com/jswrenn/stiki">Stiki repo</a>, but the scripts in the <a href="https://github.com/liber-brunoniana/liber-brunoniana">Liber Brunoniana repo</a> reflect the '1.0' rendition of the Stiki principals.</p>
  </section>

  <section><h3>Bash as a Template Engine</h3>
  <p>todo</p>
  </section>

  <section><h3>Build Process</h3>
  <p>todo</p>
  </section>
</section>

<section><h2>Presentation</h2>
todo
</section>
